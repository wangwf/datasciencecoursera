{
 "metadata": {
  "name": "",
  "signature": "sha256:08359f0e842d5c4275b096375145403905f2068a06dc5fa138222f35a69697a7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " \n",
      "# Load training data\n",
      "print 'Loading training data.'\n",
      "data_train = np.loadtxt( 'training.csv', delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading training data.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pick a random seed for reproducible results. Choose wisely!\n",
      "np.random.seed(42)\n",
      "# Random number for training/validation splitting\n",
      "r =np.random.rand(data_train.shape[0])\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put Y(truth), X(data), W(weight), and I(index) into their own arrays\n",
      "print 'Assigning data to numpy arrays.'\n",
      "# First 90% are training\n",
      "Y_train = data_train[:,32][r<0.9]\n",
      "X_train = data_train[:,1:31][r<0.9]\n",
      "W_train = data_train[:,31][r<0.9]\n",
      "# Lirst 10% are validation\n",
      "Y_valid = data_train[:,32][r>=0.9]\n",
      "X_valid = data_train[:,1:31][r>=0.9]\n",
      "W_valid = data_train[:,31][r>=0.9]\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Assigning data to numpy arrays.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tuningGBC():\n",
      "    tuned_parameters =[{'n_estimators':[50],'max_depth':[5,10,12,15],'min_samples_leaf':[150,200,250],'max_features':[10, 15, 20]}]\n",
      "    scores = ['precision', 'recall']\n",
      "    from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "    for score in scores:\n",
      "        clf =GridSearchCV(GBC(verbose=1), tuned_parameters, scoring=score)\n",
      "        clf.fit(X_train, Y_train)\n",
      "\n",
      "        print(\"Best parameters set found on development set:\")\n",
      "        print()\n",
      "        print(clf.best_estimator_)\n",
      "        print()\n",
      "        print(\"Grid scores on development set:\")\n",
      "        print()\n",
      "        for params, mean_score, scores in clf.grid_scores_:\n",
      "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "              % (mean_score, scores.std() / 2, params))\n",
      "        print()\n",
      "\n",
      "        \n",
      "#tuningGBC()\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tuningSVM():\n",
      "    # Set the parameters by cross-validation\n",
      "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-4],\n",
      "                         'C': [1]},\n",
      "                        {'kernel': ['linear'], 'C': [1]}]\n",
      "\n",
      "    scores = ['precision', 'recall']\n",
      "    from sklearn.svm import SVC\n",
      "\n",
      "    for score in scores:\n",
      "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
      "        print()\n",
      "\n",
      "        clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)\n",
      "        clf.fit(X_train, Y_train)\n",
      "\n",
      "        print(\"Best parameters set found on development set:\")\n",
      "        print()\n",
      "        print(clf.best_estimator_)\n",
      "        print()\n",
      "        print(\"Grid scores on development set:\")\n",
      "        print()\n",
      "        for params, mean_score, scores in clf.grid_scores_:\n",
      "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "                  % (mean_score, scores.std() / 2, params))\n",
      "        print()\n",
      "\n",
      "#tuningSVM()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train the GradientBoostingClassifier using our good features\n",
      "print 'Training classifier (this may take some time!)'\n",
      "#gbc = GBC(n_estimators=50, max_depth=12,min_samples_leaf=200,max_features=10,verbose=1)\n",
      "#gbc = GBC(n_estimators=50,learning_rate=0.1, max_depth=12, max_features=10, min_samples_leaf=250,verbose=1)\n",
      "gbc = GBC(learning_rate=0.1, loss='deviance', max_depth=15, max_features=10, min_samples_leaf=150,verbose=1)\n",
      "gbc.fit(X_train,Y_train) \n",
      "\n",
      "#from sklearn.ensemble import RandomForestClassifier\n",
      "#gbf = RandomForestClassifier(n_estimators=10)\n",
      "#gbf = gbf.fit(X_train, Y_train)  #clf\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training classifier (this may take some time!)\n",
        "      Iter       Train Loss   Remaining Time \n",
        "         1           1.1969            7.41m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         2           1.1244            7.38m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         3           1.0650            7.41m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         4           1.0143            7.30m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         5           0.9711            7.30m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         6           0.9345            7.21m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         7           0.9021            7.12m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         8           0.8736            7.04m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         9           0.8488            6.96m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        10           0.8268            6.89m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        20           0.7000            6.14m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        30           0.6452            5.39m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        40           0.6123            4.66m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        50           0.5900            3.93m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        60           0.5722            3.20m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        70           0.5592            2.46m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        80           0.5500            1.69m"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        90           0.5401           51.64s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       100           0.5321            0.00s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
        "              max_depth=15, max_features=10, min_samples_leaf=150,\n",
        "              min_samples_split=2, n_estimators=100, random_state=None,\n",
        "              subsample=1.0, verbose=1)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the probaility output from the trained method, using the 10% for testing\n",
      "prob_predict_train = gbc.predict_proba(X_train)[:,1]\n",
      "prob_predict_valid = gbc.predict_proba(X_valid)[:,1]\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Experience shows me that choosing the top 15% as signal gives a good AMS score.\n",
      "# This can be optimized though!\n",
      "pcut = np.percentile(prob_predict_train,85)\n",
      " \n",
      "# This are the final signal and background predictions\n",
      "Yhat_train = prob_predict_train > pcut \n",
      "Yhat_valid = prob_predict_valid > pcut\n",
      " \n",
      "# To calculate the AMS data, first get the true positives and true negatives\n",
      "# Scale the weights according to the r cutoff.\n",
      "TruePositive_train = W_train*(Y_train==1.0)*(1.0/0.9)\n",
      "TrueNegative_train = W_train*(Y_train==0.0)*(1.0/0.9)\n",
      "TruePositive_valid = W_valid*(Y_valid==1.0)*(1.0/0.1)\n",
      "TrueNegative_valid = W_valid*(Y_valid==0.0)*(1.0/0.1)\n",
      " \n",
      "# s and b for the training \n",
      "s_train = sum ( TruePositive_train*(Yhat_train==1.0) )\n",
      "b_train = sum ( TrueNegative_train*(Yhat_train==1.0) )\n",
      "s_valid = sum ( TruePositive_valid*(Yhat_valid==1.0) )\n",
      "b_valid = sum ( TrueNegative_valid*(Yhat_valid==1.0) )\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Now calculate the AMS scores\n",
      "print 'Calculating AMS score for a probability cutoff pcut=',pcut\n",
      "def AMSScore(s,b): return  math.sqrt (2.*( (s + b + 10.)*math.log(1.+s/(b+10.))-s))\n",
      "print '   - AMS based on 90% training   sample:',AMSScore(s_train,b_train)\n",
      "print '   - AMS based on 10% validation sample:',AMSScore(s_valid,b_valid)\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Calculating AMS score for a probability cutoff pcut= 0.853836778599\n",
        "   - AMS based on 90% training   sample: 6.64886092013\n",
        "   - AMS based on 10% validation sample: 3.80546703898\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Now we load the testing data, storing the data (X) and index (I)\n",
      "print 'Loading testing data'\n",
      "data_test = np.loadtxt( 'test.csv', delimiter=',', skiprows=1 )\n",
      "X_test = data_test[:,2:31]\n",
      "I_test = list(data_test[:,0])\n",
      " \n",
      "# Get a vector of the probability predictions which will be used for the ranking\n",
      "print 'Building predictions'\n",
      "Predictions_test = gbc.predict_proba(X_test)[:,1]\n",
      "# Assign labels based the best pcut\n",
      "Label_test = list(Predictions_test>pcut)\n",
      "Predictions_test =list(Predictions_test)\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading testing data\n",
        "Building predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "X.shape[1] should be 30, not 29.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-15-ec3e192eb9da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Get a vector of the probability predictions which will be used for the ranking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Building predictions'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mPredictions_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# Assign labels based the best pcut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mLabel_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPredictions_test\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mpcut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mordered\u001b[0m \u001b[0mby\u001b[0m \u001b[0marithmetical\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \"\"\"\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_score_to_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[0mpredict_stages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_init_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n\u001b[1;32m--> 656\u001b[1;33m                 self.n_features, X.shape[1]))\n\u001b[0m\u001b[0;32m    657\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: X.shape[1] should be 30, not 29."
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Now we get the CSV data, using the probability prediction in place of the ranking\n",
      "print 'Organizing the prediction results'\n",
      "resultlist = []\n",
      "for x in range(len(I_test)):\n",
      "    resultlist.append([int(I_test[x]), Predictions_test[x], 's'*(Label_test[x]==1.0)+'b'*(Label_test[x]==0.0)])\n",
      " \n",
      "# Sort the result list by the probability prediction\n",
      "resultlist = sorted(resultlist, key=lambda a_entry: a_entry[1]) \n",
      " \n",
      "# Loop over result list and replace probability prediction with integer ranking\n",
      "for y in range(len(resultlist)):\n",
      "    resultlist[y][1]=y+1\n",
      " \n",
      "# Re-sort the result list according to the index\n",
      "resultlist = sorted(resultlist, key=lambda a_entry: a_entry[0])\n",
      " \n",
      "# Write the result list data to a csv file\n",
      "print 'Writing a final csv file Kaggle_higgs_prediction_output.csv'\n",
      "fcsv = open('Kaggle_higgs_prediction_output.csv','w')\n",
      "fcsv.write('EventId,RankOrder,Class\\n')\n",
      "for line in resultlist:\n",
      "    theline = str(line[0])+','+str(line[1])+','+line[2]+'\\n'\n",
      "    fcsv.write(theline) \n",
      "fcsv.close()\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt\n",
      " \n",
      "Classifier_training_S = gbc.predict_proba(X_train[Y_train>0.5])[:,1].ravel()\n",
      "Classifier_training_B = gbc.predict_proba(X_train[Y_train<0.5])[:,1].ravel()\n",
      "Classifier_testing_A = gbc.predict_proba(X_test)[:,1].ravel()\n",
      "  \n",
      "c_max = max([Classifier_training_S.max(),Classifier_training_B.max(),Classifier_testing_A.max()])\n",
      "c_min = min([Classifier_training_S.min(),Classifier_training_B.min(),Classifier_testing_A.min()])\n",
      "  \n",
      "# Get histograms of the classifiers\n",
      "Histo_training_S = np.histogram(Classifier_training_S,bins=50,range=(c_min,c_max))\n",
      "Histo_training_B = np.histogram(Classifier_training_B,bins=50,range=(c_min,c_max))\n",
      "Histo_testing_A = np.histogram(Classifier_testing_A,bins=50,range=(c_min,c_max))\n",
      "  \n",
      "# Lets get the min/max of the Histograms\n",
      "AllHistos= [Histo_training_S,Histo_training_B]\n",
      "h_max = max([histo[0].max() for histo in AllHistos])*1.2\n",
      "# h_min = max([histo[0].min() for histo in AllHistos])\n",
      "h_min = 1.0\n",
      "  \n",
      "# Get the histogram properties (binning, widths, centers)\n",
      "bin_edges = Histo_training_S[1]\n",
      "bin_centers = ( bin_edges[:-1] + bin_edges[1:]  ) /2.\n",
      "bin_widths = (bin_edges[1:] - bin_edges[:-1])\n",
      "  \n",
      "# To make error bar plots for the data, take the Poisson uncertainty sqrt(N)\n",
      "ErrorBar_testing_A = np.sqrt(Histo_testing_A[0])\n",
      "# ErrorBar_testing_B = np.sqrt(Histo_testing_B[0])\n",
      "  \n",
      "# Draw objects\n",
      "ax1 = plt.subplot(111)\n",
      "  \n",
      "# Draw solid histograms for the training data\n",
      "ax1.bar(bin_centers-bin_widths/2.,Histo_training_B[0],facecolor='red',linewidth=0,width=bin_widths,label='B (Train)',alpha=0.5)\n",
      "ax1.bar(bin_centers-bin_widths/2.,Histo_training_S[0],bottom=Histo_training_B[0],facecolor='blue',linewidth=0,width=bin_widths,label='S (Train)',alpha=0.5)\n",
      " \n",
      "ff = (1.0*(sum(Histo_training_S[0])+sum(Histo_training_B[0])))/(1.0*sum(Histo_testing_A[0]))\n",
      " \n",
      "# # Draw error-bar histograms for the testing data\n",
      "ax1.errorbar(bin_centers, ff*Histo_testing_A[0], yerr=ff*ErrorBar_testing_A, xerr=None, ecolor='black',c='black',fmt='.',label='Test (reweighted)')\n",
      "# ax1.errorbar(bin_centers, Histo_testing_B[0], yerr=ErrorBar_testing_B, xerr=None, ecolor='red',c='red',fmt='o',label='B (Test)')\n",
      "  \n",
      "# Make a colorful backdrop to show the clasification regions in red and blue\n",
      "ax1.axvspan(pcut, c_max, color='blue',alpha=0.08)\n",
      "ax1.axvspan(c_min,pcut, color='red',alpha=0.08)\n",
      "  \n",
      "# Adjust the axis boundaries (just cosmetic)\n",
      "ax1.axis([c_min, c_max, h_min, h_max])\n",
      "  \n",
      "# Make labels and title\n",
      "plt.title(\"Higgs Kaggle Signal-Background Separation\")\n",
      "plt.xlabel(\"Probability Output (Gradient Boosting)\")\n",
      "plt.ylabel(\"Counts/Bin\")\n",
      " \n",
      "# Make legend with smalll font\n",
      "legend = ax1.legend(loc='upper center', shadow=True,ncol=2)\n",
      "for alabel in legend.get_texts():\n",
      "            alabel.set_fontsize('small')\n",
      "  \n",
      "# Save the result to png\n",
      "plt.savefig(\"Sklearn_gbc.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}